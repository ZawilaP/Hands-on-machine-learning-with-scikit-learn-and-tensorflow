{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from functools import wraps\n",
    "\n",
    "def timer_measurer(orig_func):\n",
    "    import time\n",
    "\n",
    "    @wraps(orig_func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = orig_func(*args, **kwargs)\n",
    "        end = time.time() - start\n",
    "        print(f'{orig_func.__name__} ran in: {end} sec')\n",
    "        return result\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a custom layer that performs Layer Normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLayerNormalizer(keras.layers.Layer):\n",
    "    def __init__(self,*args ,**kwargs):\n",
    "        super(MyLayerNormalizer, self).__init__(*args, **kwargs)\n",
    "        self.eps = 0.001\n",
    "        \n",
    "    def build(self, batch_input_shape):\n",
    "        self.alpha = self.add_weight(\n",
    "            name=\"alpha\", shape = batch_input_shape[-1:], initializer = \"ones\", dtype=tf.float32)\n",
    "        self.beta = self.add_weight(\n",
    "            name=\"beta\", shape = batch_input_shape[-1:], initializer = \"zeros\", dtype=tf.float32)\n",
    "        super(MyLayerNormalizer, self).build(batch_input_shape)\n",
    "        \n",
    "    def call(self, X):\n",
    "        mean, variance = tf.nn.moments(X, axes=-1, keepdims=True)\n",
    "        sd = tf.math.sqrt(variance)\n",
    "        centralized_mean = X - mean\n",
    "        denominator = sd + self.eps\n",
    "        right_side = centralized_mean / denominator\n",
    "        output = tf.math.multiply(self.alpha, right_side) + self.beta\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that it produces similar results like keras.layers.LayerNormalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer my_layer_normalizer is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer layer_normalization is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=75, shape=(404, 13), dtype=bool, numpy=\n",
       "array([[ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       ...,\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True]])>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_boston_train, y_boston_train), _ = keras.datasets.boston_housing.load_data()\n",
    "\n",
    "mylayer = MyLayerNormalizer()\n",
    "my_results = mylayer(x_boston_train)\n",
    "\n",
    "norm_layer = keras.layers.LayerNormalization()\n",
    "norm_results = norm_layer(x_boston_train, training=True)\n",
    "\n",
    "tf.less_equal(abs(my_results-norm_results), 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking manually differences between first observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=79, shape=(13,), dtype=float32, numpy=\n",
       "array([-0.517459  , -0.52729   , -0.4623597 , -0.52729   , -0.5229985 ,\n",
       "       -0.47829714,  0.20417313, -0.49556747, -0.4953832 ,  1.9215562 ,\n",
       "       -0.35977936,  2.6386611 , -0.37796623], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=83, shape=(13,), dtype=float32, numpy=\n",
       "array([-0.517463  , -0.52729416, -0.46236333, -0.52729416, -0.5230027 ,\n",
       "       -0.4783009 ,  0.2041747 , -0.49557137, -0.4953871 ,  1.9215713 ,\n",
       "       -0.3597822 ,  2.638682  , -0.3779692 ], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a model using a custom training loop to tackle the Fashion MNIST dataset:\n",
    "\n",
    "Display the epoch, iteration, mean training loss, and mean accuracy over each epoch (updated at each iteration), as well as the validation loss and accuracy at the end of each epoch\n",
    "\n",
    "Try using a different optimizer with a different learning rate for the upper layers and the lower layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "\n",
    "# Data loading and preparation:\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(x_train_full, y_train_full), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "x_validation, x_train = x_train_full[:5000] / 255.0, x_train_full[5000:] / 255.0\n",
    "y_validation, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Creating model:\n",
    "l2_reg = keras.regularizers.l2(0.05)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape = x_train.shape[1:]))\n",
    "model.add(keras.layers.Dense(300, activation = \"relu\", kernel_initializer = \"he_normal\", kernel_regularizer = l2_reg))\n",
    "model.add(keras.layers.Dense(100, activation = \"relu\", kernel_initializer = \"he_normal\", kernel_regularizer = l2_reg))\n",
    "model.add(keras.layers.Dense(100, activation = \"relu\", kernel_initializer = \"he_normal\", kernel_regularizer = l2_reg))\n",
    "model.add(keras.layers.Dense(100, activation = \"relu\", kernel_initializer = \"he_normal\", kernel_regularizer = l2_reg))\n",
    "model.add(keras.layers.Dense(10, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets compile this in a conventional keras way, and later we will do it ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/50\n",
      "55000/55000 [==============================] - 3s 59us/sample - loss: 16.9542 - accuracy: 0.6661 - val_loss: 3.7396 - val_accuracy: 0.7276\n",
      "Epoch 2/50\n",
      "55000/55000 [==============================] - 3s 49us/sample - loss: 2.3846 - accuracy: 0.6967 - val_loss: 1.7398 - val_accuracy: 0.6938\n",
      "Epoch 3/50\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 1.5453 - accuracy: 0.7013 - val_loss: 1.3833 - val_accuracy: 0.7334\n",
      "Epoch 4/50\n",
      "55000/55000 [==============================] - 2s 45us/sample - loss: 1.3928 - accuracy: 0.7133 - val_loss: 1.3225 - val_accuracy: 0.7356\n",
      "Epoch 5/50\n",
      "55000/55000 [==============================] - 2s 44us/sample - loss: 1.3309 - accuracy: 0.7229 - val_loss: 1.2651 - val_accuracy: 0.7376\n",
      "Epoch 6/50\n",
      "55000/55000 [==============================] - 2s 44us/sample - loss: 1.2861 - accuracy: 0.7273 - val_loss: 1.2251 - val_accuracy: 0.7498\n",
      "Epoch 7/50\n",
      "55000/55000 [==============================] - 2s 43us/sample - loss: 1.2495 - accuracy: 0.7338 - val_loss: 1.2139 - val_accuracy: 0.7500\n",
      "Epoch 8/50\n",
      "55000/55000 [==============================] - 2s 44us/sample - loss: 1.2233 - accuracy: 0.7402 - val_loss: 1.2522 - val_accuracy: 0.7204\n",
      "Epoch 9/50\n",
      "55000/55000 [==============================] - 3s 46us/sample - loss: 1.1964 - accuracy: 0.7445 - val_loss: 1.1556 - val_accuracy: 0.7536\n",
      "Epoch 10/50\n",
      "55000/55000 [==============================] - 3s 46us/sample - loss: 1.1801 - accuracy: 0.7455 - val_loss: 1.1736 - val_accuracy: 0.7506\n",
      "Epoch 11/50\n",
      "55000/55000 [==============================] - 3s 46us/sample - loss: 1.1599 - accuracy: 0.7514 - val_loss: 1.1411 - val_accuracy: 0.7600\n",
      "Epoch 12/50\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 1.1447 - accuracy: 0.7515 - val_loss: 1.1466 - val_accuracy: 0.7342\n",
      "Epoch 13/50\n",
      "55000/55000 [==============================] - 2s 45us/sample - loss: 1.1265 - accuracy: 0.7570 - val_loss: 1.1085 - val_accuracy: 0.7690\n",
      "Epoch 14/50\n",
      "55000/55000 [==============================] - 2s 44us/sample - loss: 1.1150 - accuracy: 0.7580 - val_loss: 1.0813 - val_accuracy: 0.7766\n",
      "Epoch 15/50\n",
      "55000/55000 [==============================] - 3s 46us/sample - loss: 1.1013 - accuracy: 0.7610 - val_loss: 1.0781 - val_accuracy: 0.7760\n",
      "Epoch 16/50\n",
      "55000/55000 [==============================] - ETA: 0s - loss: 1.0904 - accuracy: 0.76 - 3s 48us/sample - loss: 1.0907 - accuracy: 0.7619 - val_loss: 1.0753 - val_accuracy: 0.7640\n",
      "Epoch 17/50\n",
      "55000/55000 [==============================] - 3s 46us/sample - loss: 1.0820 - accuracy: 0.7648 - val_loss: 1.0561 - val_accuracy: 0.7792\n",
      "Epoch 18/50\n",
      "55000/55000 [==============================] - 3s 55us/sample - loss: 1.0699 - accuracy: 0.7666 - val_loss: 1.0398 - val_accuracy: 0.7794\n",
      "Epoch 19/50\n",
      "55000/55000 [==============================] - 3s 51us/sample - loss: 1.0624 - accuracy: 0.7686 - val_loss: 1.0439 - val_accuracy: 0.7804\n",
      "Epoch 20/50\n",
      "55000/55000 [==============================] - 3s 53us/sample - loss: 1.0515 - accuracy: 0.7708 - val_loss: 1.0372 - val_accuracy: 0.7858\n",
      "Epoch 21/50\n",
      "55000/55000 [==============================] - 3s 55us/sample - loss: 1.0448 - accuracy: 0.7712 - val_loss: 1.0133 - val_accuracy: 0.7878\n",
      "Epoch 22/50\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 1.0357 - accuracy: 0.7731 - val_loss: 1.0274 - val_accuracy: 0.7696\n",
      "Epoch 23/50\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 1.0297 - accuracy: 0.7726 - val_loss: 1.0000 - val_accuracy: 0.7902\n",
      "Epoch 24/50\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 1.0265 - accuracy: 0.7719 - val_loss: 1.0146 - val_accuracy: 0.7712\n",
      "Epoch 25/50\n",
      "55000/55000 [==============================] - 2s 45us/sample - loss: 1.0169 - accuracy: 0.7755 - val_loss: 1.0064 - val_accuracy: 0.7746\n",
      "Epoch 26/50\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 1.0105 - accuracy: 0.7774 - val_loss: 1.0084 - val_accuracy: 0.7828\n",
      "Epoch 27/50\n",
      "55000/55000 [==============================] - 3s 49us/sample - loss: 1.0047 - accuracy: 0.7775 - val_loss: 1.0113 - val_accuracy: 0.7818\n",
      "Epoch 28/50\n",
      "55000/55000 [==============================] - 3s 54us/sample - loss: 0.9991 - accuracy: 0.7785 - val_loss: 0.9838 - val_accuracy: 0.7882\n",
      "Epoch 29/50\n",
      "55000/55000 [==============================] - 3s 51us/sample - loss: 0.9958 - accuracy: 0.7787 - val_loss: 0.9838 - val_accuracy: 0.7886\n",
      "Epoch 30/50\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 0.9892 - accuracy: 0.7805 - val_loss: 0.9670 - val_accuracy: 0.7950\n",
      "Epoch 31/50\n",
      "55000/55000 [==============================] - 3s 54us/sample - loss: 0.9844 - accuracy: 0.7805 - val_loss: 0.9558 - val_accuracy: 0.7956\n",
      "Epoch 32/50\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.9805 - accuracy: 0.7811 - val_loss: 0.9555 - val_accuracy: 0.8016\n",
      "Epoch 33/50\n",
      "55000/55000 [==============================] - 3s 59us/sample - loss: 0.9749 - accuracy: 0.7837 - val_loss: 0.9619 - val_accuracy: 0.7932\n",
      "Epoch 34/50\n",
      "55000/55000 [==============================] - 3s 59us/sample - loss: 0.9709 - accuracy: 0.7849 - val_loss: 0.9610 - val_accuracy: 0.7936\n",
      "Epoch 35/50\n",
      "55000/55000 [==============================] - 3s 56us/sample - loss: 0.9656 - accuracy: 0.7848 - val_loss: 0.9458 - val_accuracy: 0.7990\n",
      "Epoch 36/50\n",
      "55000/55000 [==============================] - 3s 59us/sample - loss: 0.9634 - accuracy: 0.7839 - val_loss: 0.9438 - val_accuracy: 0.7962\n",
      "Epoch 37/50\n",
      "55000/55000 [==============================] - 3s 61us/sample - loss: 0.9560 - accuracy: 0.7875 - val_loss: 0.9420 - val_accuracy: 0.8022\n",
      "Epoch 38/50\n",
      "55000/55000 [==============================] - 3s 52us/sample - loss: 0.9539 - accuracy: 0.7889 - val_loss: 0.9348 - val_accuracy: 0.8032\n",
      "Epoch 39/50\n",
      "55000/55000 [==============================] - 3s 54us/sample - loss: 0.9492 - accuracy: 0.7894 - val_loss: 0.9230 - val_accuracy: 0.8042\n",
      "Epoch 40/50\n",
      "55000/55000 [==============================] - 3s 52us/sample - loss: 0.9438 - accuracy: 0.7905 - val_loss: 0.9291 - val_accuracy: 0.8018\n",
      "Epoch 41/50\n",
      "55000/55000 [==============================] - 3s 55us/sample - loss: 0.9405 - accuracy: 0.7922 - val_loss: 0.9183 - val_accuracy: 0.8050\n",
      "Epoch 42/50\n",
      "55000/55000 [==============================] - 3s 55us/sample - loss: 0.9358 - accuracy: 0.7921 - val_loss: 0.9133 - val_accuracy: 0.8062\n",
      "Epoch 43/50\n",
      "55000/55000 [==============================] - 3s 53us/sample - loss: 0.9346 - accuracy: 0.7915 - val_loss: 0.9539 - val_accuracy: 0.7826\n",
      "Epoch 44/50\n",
      "55000/55000 [==============================] - 3s 57us/sample - loss: 0.9305 - accuracy: 0.7937 - val_loss: 0.9038 - val_accuracy: 0.8078\n",
      "Epoch 45/50\n",
      "55000/55000 [==============================] - 3s 55us/sample - loss: 0.9267 - accuracy: 0.7946 - val_loss: 0.9077 - val_accuracy: 0.8082\n",
      "Epoch 46/50\n",
      "55000/55000 [==============================] - 3s 56us/sample - loss: 0.9232 - accuracy: 0.7959 - val_loss: 0.9159 - val_accuracy: 0.7980\n",
      "Epoch 47/50\n",
      "55000/55000 [==============================] - 3s 58us/sample - loss: 0.9203 - accuracy: 0.7972 - val_loss: 0.9101 - val_accuracy: 0.8052\n",
      "Epoch 48/50\n",
      "55000/55000 [==============================] - 3s 57us/sample - loss: 0.9181 - accuracy: 0.7955 - val_loss: 0.9002 - val_accuracy: 0.8104\n",
      "Epoch 49/50\n",
      "55000/55000 [==============================] - 3s 61us/sample - loss: 0.9121 - accuracy: 0.7985 - val_loss: 0.9021 - val_accuracy: 0.8048\n",
      "Epoch 50/50\n",
      "55000/55000 [==============================] - 3s 60us/sample - loss: 0.9109 - accuracy: 0.7998 - val_loss: 0.9211 - val_accuracy: 0.7968\n",
      "[0.773391241645813, 0.8106]\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=keras.losses.sparse_categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Nadam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=256,\n",
    "          epochs=50,\n",
    "          verbose=1,\n",
    "          validation_data=(x_validation, y_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to do this whole training loop myself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "n_epochs = 50\n",
    "n_steps = len(x_train) // batch_size\n",
    "loss_fn = keras.losses.sparse_categorical_crossentropy\n",
    "optimizer1 = keras.optimizers.Nadam()\n",
    "# optimizer2 = keras.optimizers.SGD(nesterov = True)\n",
    "optimizer2 = optimizer1\n",
    "mean_loss = keras.metrics.Mean()\n",
    "metrics = [keras.metrics.Accuracy()]\n",
    "val_metrics = [keras.metrics.Accuracy()]\n",
    "\n",
    "model2 = keras.models.Sequential()\n",
    "model2.add(keras.layers.Flatten(input_shape = x_train.shape[1:]))\n",
    "model2.add(keras.layers.Dense(300, activation = \"relu\", kernel_initializer = \"he_normal\", kernel_regularizer = l2_reg))\n",
    "model2.add(keras.layers.Dense(100, activation = \"relu\", kernel_initializer = \"he_normal\", kernel_regularizer = l2_reg))\n",
    "model2.add(keras.layers.Dense(100, activation = \"relu\", kernel_initializer = \"he_normal\", kernel_regularizer = l2_reg))\n",
    "model2.add(keras.layers.Dense(100, activation = \"relu\", kernel_initializer = \"he_normal\", kernel_regularizer = l2_reg))\n",
    "model2.add(keras.layers.Dense(10, activation = \"softmax\"))\n",
    "\n",
    "def random_batch(x, y, batch_size = batch_size):\n",
    "    \"\"\"\n",
    "    Get random batch of data for training\n",
    "    \"\"\"\n",
    "    idx = np.random.randint(len(x), size=batch_size)\n",
    "    return x[idx], y[idx]\n",
    "\n",
    "def print_status_bar(iteration, total, loss, metrics=None, valid_loss = None, valid_metrics = None):\n",
    "    \"\"\"\n",
    "    Print the status bar\n",
    "    \"\"\"\n",
    "    metrics_print = \"loss: {:.4f} - \".format(loss.result() ) + \" - \".join([\"{}: {:.4f}\".format(m.name, m.result()) for m in (metrics or [])])\n",
    "    if valid_loss is not None:\n",
    "        valid_print = \" - valid_loss: {:.4f} - \".format(valid_loss.numpy() ) + \" - \".join([\"valid_{}: {:.4f}\".format(m.name, m.result()) for m in (valid_metrics or [])])\n",
    "        metrics_print = metrics_print + valid_print\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{}/{} - \".format(iteration, total) + metrics_print, end=end)\n",
    "\n",
    "def compute_metrics(metrics, y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the results for each metric in the list.\n",
    "    \"\"\"\n",
    "    # take the max in y_pred\n",
    "    y_pred = tf.math.argmax(y_pred, axis=-1)\n",
    "    for metric in metrics:\n",
    "        metric(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer_measurer\n",
    "def train_neural_network():\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        print(f\"Epoch {epoch}/{n_epochs}\")\n",
    "        for step in range(1, n_steps + 1):\n",
    "            x_batch, y_batch = random_batch(x_train, y_train)\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred = model2(x_batch, training=True)\n",
    "                main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "                loss = tf.add_n([main_loss] + model2.losses)\n",
    "            gradients = tape.gradient(loss, model2.trainable_variables)\n",
    "            gradients1, variables1 = gradients[:math.floor(len(model2.layers)/2)], model2.trainable_variables[:math.floor(len(model2.layers)/2)]\n",
    "            gradients2, variables2 = gradients[math.ceil(len(model2.layers)/2):], model2.trainable_variables[math.ceil(len(model2.layers)/2):]\n",
    "            optimizer1.apply_gradients(zip(gradients1, variables1))\n",
    "            optimizer2.apply_gradients(zip(gradients2, variables2))\n",
    "            mean_loss(loss)\n",
    "            compute_metrics(metrics, y_batch, y_pred)\n",
    "            print_status_bar(step * batch_size, len(y_train), mean_loss, metrics)\n",
    "        # Now to get validation results:\n",
    "        y_pred_valid = model2(x_validation, training = False)\n",
    "        validation_loss = tf.reduce_mean(loss_fn(y_validation, y_pred_valid))\n",
    "        compute_metrics(val_metrics, y_batch, y_pred)\n",
    "        print_status_bar(len(y_train), len(y_train), mean_loss, metrics, validation_loss, val_metrics)\n",
    "        for metric in [mean_loss] + metrics:\n",
    "            metric.reset_states()\n",
    "            \n",
    "    return model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer flatten_15 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "55000/55000 - loss: 13.6983 - accuracy: 0.6595 - valid_loss: 0.9665 - valid_accuracy: 0.6836\n",
      "Epoch 2/50\n",
      "55000/55000 - loss: 1.8391 - accuracy: 0.6757 - valid_loss: 0.8516 - valid_accuracy: 0.6719\n",
      "Epoch 3/50\n",
      "55000/55000 - loss: 1.4375 - accuracy: 0.6947 - valid_loss: 0.7803 - valid_accuracy: 0.7057\n",
      "Epoch 4/50\n",
      "55000/55000 - loss: 1.3275 - accuracy: 0.7158 - valid_loss: 0.8135 - valid_accuracy: 0.6953\n",
      "Epoch 5/50\n",
      "55000/55000 - loss: 1.2728 - accuracy: 0.7278 - valid_loss: 0.7910 - valid_accuracy: 0.7016\n",
      "Epoch 6/50\n",
      "55000/55000 - loss: 1.2341 - accuracy: 0.7363 - valid_loss: 0.7050 - valid_accuracy: 0.7116\n",
      "Epoch 7/50\n",
      "55000/55000 - loss: 1.1924 - accuracy: 0.7457 - valid_loss: 0.6886 - valid_accuracy: 0.7176\n",
      "Epoch 8/50\n",
      "55000/55000 - loss: 1.1691 - accuracy: 0.7458 - valid_loss: 0.6845 - valid_accuracy: 0.7192\n",
      "Epoch 9/50\n",
      "55000/55000 - loss: 1.1438 - accuracy: 0.7562 - valid_loss: 0.6787 - valid_accuracy: 0.7248\n",
      "Epoch 10/50\n",
      "55000/55000 - loss: 1.1318 - accuracy: 0.7553 - valid_loss: 0.6555 - valid_accuracy: 0.7289\n",
      "Epoch 11/50\n",
      "55000/55000 - loss: 1.1146 - accuracy: 0.7599 - valid_loss: 0.6643 - valid_accuracy: 0.7308\n",
      "Epoch 12/50\n",
      "55000/55000 - loss: 1.1022 - accuracy: 0.7624 - valid_loss: 0.6503 - valid_accuracy: 0.7354\n",
      "Epoch 13/50\n",
      "55000/55000 - loss: 1.0873 - accuracy: 0.7641 - valid_loss: 0.6399 - valid_accuracy: 0.7368\n",
      "Epoch 14/50\n",
      "55000/55000 - loss: 1.0713 - accuracy: 0.7715 - valid_loss: 0.6662 - valid_accuracy: 0.7374\n",
      "Epoch 15/50\n",
      "55000/55000 - loss: 1.0626 - accuracy: 0.7725 - valid_loss: 0.6326 - valid_accuracy: 0.7409\n",
      "Epoch 16/50\n",
      "55000/55000 - loss: 1.0549 - accuracy: 0.7732 - valid_loss: 0.6543 - valid_accuracy: 0.7380\n",
      "Epoch 17/50\n",
      "55000/55000 - loss: 1.0459 - accuracy: 0.7718 - valid_loss: 0.6186 - valid_accuracy: 0.7415\n",
      "Epoch 18/50\n",
      "55000/55000 - loss: 1.0438 - accuracy: 0.7718 - valid_loss: 0.6268 - valid_accuracy: 0.7454\n",
      "Epoch 19/50\n",
      "55000/55000 - loss: 1.0295 - accuracy: 0.7762 - valid_loss: 0.6321 - valid_accuracy: 0.7473\n",
      "Epoch 20/50\n",
      "55000/55000 - loss: 1.0257 - accuracy: 0.7753 - valid_loss: 0.6221 - valid_accuracy: 0.7471\n",
      "Epoch 21/50\n",
      "55000/55000 - loss: 1.0190 - accuracy: 0.7773 - valid_loss: 0.6110 - valid_accuracy: 0.7485\n",
      "Epoch 22/50\n",
      "55000/55000 - loss: 1.0137 - accuracy: 0.7749 - valid_loss: 0.6149 - valid_accuracy: 0.7491\n",
      "Epoch 23/50\n",
      "55000/55000 - loss: 1.0019 - accuracy: 0.7785 - valid_loss: 0.6191 - valid_accuracy: 0.7498\n",
      "Epoch 24/50\n",
      "55000/55000 - loss: 0.9948 - accuracy: 0.7827 - valid_loss: 0.6143 - valid_accuracy: 0.7490\n",
      "Epoch 25/50\n",
      "55000/55000 - loss: 0.9876 - accuracy: 0.7847 - valid_loss: 0.6087 - valid_accuracy: 0.7508\n",
      "Epoch 26/50\n",
      "55000/55000 - loss: 0.9865 - accuracy: 0.7858 - valid_loss: 0.6171 - valid_accuracy: 0.7512\n",
      "Epoch 27/50\n",
      "55000/55000 - loss: 0.9836 - accuracy: 0.7838 - valid_loss: 0.5961 - valid_accuracy: 0.7522\n",
      "Epoch 28/50\n",
      "55000/55000 - loss: 0.9800 - accuracy: 0.7841 - valid_loss: 0.6021 - valid_accuracy: 0.7532\n",
      "Epoch 29/50\n",
      "55000/55000 - loss: 0.9746 - accuracy: 0.7840 - valid_loss: 0.6000 - valid_accuracy: 0.7546\n",
      "Epoch 30/50\n",
      "55000/55000 - loss: 0.9656 - accuracy: 0.7870 - valid_loss: 0.5923 - valid_accuracy: 0.7557\n",
      "Epoch 31/50\n",
      "55000/55000 - loss: 0.9591 - accuracy: 0.7879 - valid_loss: 0.5929 - valid_accuracy: 0.7562\n",
      "Epoch 32/50\n",
      "55000/55000 - loss: 0.9591 - accuracy: 0.7896 - valid_loss: 0.5867 - valid_accuracy: 0.7571\n",
      "Epoch 33/50\n",
      "55000/55000 - loss: 0.9559 - accuracy: 0.7887 - valid_loss: 0.5816 - valid_accuracy: 0.7590\n",
      "Epoch 34/50\n",
      "55000/55000 - loss: 0.9484 - accuracy: 0.7922 - valid_loss: 0.5819 - valid_accuracy: 0.7599\n",
      "Epoch 35/50\n",
      "55000/55000 - loss: 0.9485 - accuracy: 0.7904 - valid_loss: 0.5789 - valid_accuracy: 0.7622\n",
      "Epoch 36/50\n",
      "55000/55000 - loss: 0.9386 - accuracy: 0.7933 - valid_loss: 0.5802 - valid_accuracy: 0.7620\n",
      "Epoch 37/50\n",
      "55000/55000 - loss: 0.9401 - accuracy: 0.7925 - valid_loss: 0.5855 - valid_accuracy: 0.7622\n",
      "Epoch 38/50\n",
      "55000/55000 - loss: 0.9329 - accuracy: 0.7933 - valid_loss: 0.5741 - valid_accuracy: 0.7636\n",
      "Epoch 39/50\n",
      "55000/55000 - loss: 0.9297 - accuracy: 0.7970 - valid_loss: 0.5788 - valid_accuracy: 0.7646\n",
      "Epoch 40/50\n",
      "55000/55000 - loss: 0.9236 - accuracy: 0.7946 - valid_loss: 0.5793 - valid_accuracy: 0.7646\n",
      "Epoch 41/50\n",
      "55000/55000 - loss: 0.9265 - accuracy: 0.7965 - valid_loss: 0.5647 - valid_accuracy: 0.7659\n",
      "Epoch 42/50\n",
      "55000/55000 - loss: 0.9161 - accuracy: 0.7994 - valid_loss: 0.5655 - valid_accuracy: 0.7661\n",
      "Epoch 43/50\n",
      "55000/55000 - loss: 0.9126 - accuracy: 0.8008 - valid_loss: 0.5611 - valid_accuracy: 0.7674\n",
      "Epoch 44/50\n",
      "55000/55000 - loss: 0.9132 - accuracy: 0.7992 - valid_loss: 0.5886 - valid_accuracy: 0.7674\n",
      "Epoch 45/50\n",
      "55000/55000 - loss: 0.9133 - accuracy: 0.8003 - valid_loss: 0.5679 - valid_accuracy: 0.7680\n",
      "Epoch 46/50\n",
      "55000/55000 - loss: 0.9049 - accuracy: 0.8024 - valid_loss: 0.5595 - valid_accuracy: 0.7699\n",
      "Epoch 47/50\n",
      "55000/55000 - loss: 0.9050 - accuracy: 0.8028 - valid_loss: 0.5543 - valid_accuracy: 0.7708\n",
      "Epoch 48/50\n",
      "55000/55000 - loss: 0.8986 - accuracy: 0.8025 - valid_loss: 0.5543 - valid_accuracy: 0.7712\n",
      "Epoch 49/50\n",
      "55000/55000 - loss: 0.8979 - accuracy: 0.8032 - valid_loss: 0.5513 - valid_accuracy: 0.7716\n",
      "Epoch 50/50\n",
      "55000/55000 - loss: 0.8970 - accuracy: 0.8021 - valid_loss: 0.5617 - valid_accuracy: 0.7720\n",
      "train_neural_network ran in: 471.70451617240906 sec\n"
     ]
    }
   ],
   "source": [
    "model2 = train_neural_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras implemented model test accuracy: 0.7793\n",
      "Manually implemented model test accuracy: 0.8094\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_1 = model.predict(x_test)\n",
    "y_pred_1 = np.argmax(y_pred_1, axis=-1)\n",
    "\n",
    "y_pred_2 = model2.predict(x_test)\n",
    "y_pred_2 = np.argmax(y_pred_2, axis=-1)\n",
    "\n",
    "print(f\"Keras implemented model test accuracy: {accuracy_score(y_test, y_pred_1)}\")    \n",
    "print(f\"Manually implemented model test accuracy: {accuracy_score(y_test, y_pred_2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
